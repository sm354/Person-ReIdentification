{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import random\n",
    "import timm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import faiss\n",
    "from PIL import Image\n",
    "# from tqdm.notebook import tqdm\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "# from model import *\n",
    "# from utils import *\n",
    "from metrics import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "# device = \"cpu\" \n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "batch_size = 8\n",
    "lr = 3e-4\n",
    "gamma = 0.7\n",
    "lmbd = 8\n",
    "model_path = \"./model/la-tf++.pth\"\n",
    "# data_dir = \"/home/shubham/CVP/data/val\"\n",
    "data_dir = \"/home/shubham/CVP/test\"\n",
    "visualization_path = \"/home/shubham/CVP/Predictions\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_query_list = [\n",
    "    transforms.Resize((224,224), interpolation=3),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]\n",
    "transform_gallery_list = [\n",
    "    transforms.Resize(size=(224,224),interpolation=3), #Image.BICUBIC\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]\n",
    "data_transforms = {\n",
    "    'query': transforms.Compose( transform_query_list ),\n",
    "    'gallery': transforms.Compose(transform_gallery_list),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_datasets = {}\n",
    "image_datasets['query'] = datasets.ImageFolder(os.path.join(data_dir, 'query'),\n",
    "                                          data_transforms['query'])\n",
    "image_datasets['gallery'] = datasets.ImageFolder(os.path.join(data_dir, 'gallery'),\n",
    "                                          data_transforms['gallery'])\n",
    "query_loader = DataLoader(dataset = image_datasets['query'], batch_size=batch_size, shuffle=False )\n",
    "gallery_loader = DataLoader(dataset = image_datasets['gallery'], batch_size=batch_size, shuffle=False)\n",
    "\n",
    "class_names = image_datasets['query'].classes\n",
    "print(len(class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LATransformer(nn.Module):\n",
    "    def __init__(self, ViT, lmbd, num_classes=751, test=False):\n",
    "        super(LATransformer, self).__init__()\n",
    "        self.test = test\n",
    "        self.class_num = num_classes # output number of classes\n",
    "        \n",
    "        # ViT model\n",
    "        self.model = ViT\n",
    "        self.model.head.requires_grad_ = False \n",
    "        self.cls_token = self.model.cls_token # 1, 1, 768\n",
    "        self.pos_embed = self.model.pos_embed # 1, 197, 768\n",
    "\n",
    "        # these are ViT model internal hyper-parameters (FIXED) \n",
    "        # self.num_blocks = 12 # number of sequential blocks in ViT\n",
    "        \n",
    "        # there are 196 patches in each image; thus, we split them into 14 x 14 grid\n",
    "        self.num_rows = 14 \n",
    "        self.num_cols = 14\n",
    "\n",
    "        # Locally aware network\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((self.num_rows,768))\n",
    "        self.lmbd = lmbd\n",
    "\n",
    "        if not self.test:\n",
    "            # ensemble of classifiers\n",
    "            # for i in range(self.num_rows):\n",
    "            #     name = 'classifier'+str(i)\n",
    "            #     setattr(self, name, FC_Classifier(input_dim=768, num_classes=self.class_num, droprate=0.5, num_bottleneck=256, return_features=False))\n",
    "            name = 'classifier'+str(0)\n",
    "            setattr(self, name, FC_Classifier(input_dim=768, num_classes=self.class_num, droprate=0.5, num_bottleneck=256, return_features=False))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape = 32, 3, 224, 224\n",
    "        \n",
    "        # Divide input image into patch embeddings and add position embeddings\n",
    "        x = self.model.patch_embed(x) # 32, 196, 768\n",
    "        cls_token = self.cls_token.expand(x.shape[0], -1, -1)  # 32, 1, 768\n",
    "        x = torch.cat((cls_token, x), dim=1) # 32, 197, 768\n",
    "        trnsfrmr_inp = self.model.pos_drop(x + self.pos_embed) # dropout with p = 0; idk!\n",
    "        \n",
    "        # Feed forward the x = (patch_embeddings+position_embeddings) through transformer blocks\n",
    "        # for i in range(self.num_blocks):\n",
    "        #    x = self.model.blocks[i](x)\n",
    "        x = self.model.blocks(trnsfrmr_inp)\n",
    "        x_trnsfrmr_encdd = self.model.norm(x) # layer normalization; shape = 32, 197, 768\n",
    "        \n",
    "        # extract the cls token\n",
    "        cls_token_out = x_trnsfrmr_encdd[:, 0].unsqueeze(1)\n",
    "        \n",
    "        # Average pool\n",
    "        Q = x_trnsfrmr_encdd[:, 1:]\n",
    "        L = self.avgpool(Q) # 32, 14, 768\n",
    "                \n",
    "        if self.test:\n",
    "            return L # moving this down the global-cls addition drops the testing score\n",
    "        \n",
    "        # Add global cls token to each local token \n",
    "        for i in range(self.num_rows):\n",
    "            out = torch.mul(L[:, i, :], self.lmbd)\n",
    "            L[:,i,:] = torch.div(torch.add(cls_token_out.squeeze(),out), 1+self.lmbd)\n",
    "            \n",
    "        L, _ = torch.max(L, dim=1)\n",
    "        \n",
    "        # Locally aware network\n",
    "        part = {}\n",
    "        predict = {}\n",
    "        # for i in range(self.num_rows):\n",
    "        #     part[i] = L[:,i,:] # 32, 768\n",
    "        #     name = 'classifier'+str(i)\n",
    "        #     c = getattr(self, name)\n",
    "        #     predict[i] = c(part[i]) # 32, 751\n",
    "        name = 'classifier'+str(0)\n",
    "        c = getattr(self, name)\n",
    "        predict[0] = c(L) # 32, 751\n",
    "\n",
    "        return predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ViT\n",
    "vit_base = timm.create_model('vit_base_patch16_224', pretrained=True, num_classes=751)\n",
    "vit_base = vit_base.to(device)\n",
    "\n",
    "# Create La-Transformer\n",
    "model = LATransformer(vit_base, lmbd=lmbd, num_classes=123, test=True).to(device)\n",
    "\n",
    "# Load LA-Transformer\n",
    "model.load_state_dict(torch.load(model_path), strict=False)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature(model, dataloaders):\n",
    "    imgs = torch.FloatTensor()\n",
    "    features = torch.FloatTensor()\n",
    "    for data in tqdm(dataloaders):\n",
    "        img, label = data\n",
    "        \n",
    "        img_copy = img.clone()\n",
    "        imgs = torch.cat((imgs, img_copy), 0)\n",
    "        \n",
    "        img, label = img.to(device), label.to(device)\n",
    "\n",
    "        output = model(img)\n",
    "        features = torch.cat((features, output.detach().cpu()), 0)\n",
    "\n",
    "    return features, imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extract Query Features\n",
    "query_feature, query_imgs = extract_feature(model, query_loader)\n",
    "\n",
    "# Extract Gallery Features\n",
    "gallery_feature, gallery_imgs = extract_feature(model, gallery_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve labels\n",
    "gallery_path = image_datasets['gallery'].imgs\n",
    "query_path = image_datasets['query'].imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gallery_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_id(img_path):\n",
    "    camera_id = []\n",
    "    labels = []\n",
    "    for path, label in img_path:\n",
    "        cam_id = int(path.split(\"/\")[-1].split(\"_\")[0])\n",
    "        # filename = os.path.basename(path)\n",
    "        # camera = filename.split('_')[0]\n",
    "        labels.append(int(label))\n",
    "        camera_id.append(cam_id)\n",
    "    return camera_id, labels\n",
    "\n",
    "gallery_cam, gallery_label = get_id(gallery_path)\n",
    "query_cam, query_label = get_id(query_path)\n",
    "\n",
    "gallery_label = np.array(gallery_label)\n",
    "query_label = np.array(query_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concat Averaged GELTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "concatenated_query_vectors = []\n",
    "for query in tqdm(query_feature):\n",
    "    fnorm = torch.norm(query, p=2, dim=1, keepdim=True)*np.sqrt(14)\n",
    "    query_norm = query.div(fnorm.expand_as(query))\n",
    "    concatenated_query_vectors.append(query_norm.view((-1))) # 14*768 -> 10752\n",
    "#     concatenated_query_vectors.append(query.view((-1)))\n",
    "\n",
    "concatenated_gallery_vectors = []\n",
    "for gallery in tqdm(gallery_feature):\n",
    "    fnorm = torch.norm(gallery, p=2, dim=1, keepdim=True) *np.sqrt(14)\n",
    "    gallery_norm = gallery.div(fnorm.expand_as(gallery))\n",
    "    concatenated_gallery_vectors.append(gallery_norm.view((-1))) # 14*768 -> 10752\n",
    "#     concatenated_gallery_vectors.append(gallery.view((-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Similarity using FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# index = faiss.IndexIDMap(faiss.IndexFlatIP(10752)) # inner product\n",
    "# index.add_with_ids(np.array([t.numpy() for t in concatenated_gallery_vectors]),np.array(gallery_label))\n",
    "index = faiss.IndexFlatIP(10752) # inner product\n",
    "index.add(np.array([t.numpy() for t in concatenated_gallery_vectors]))\n",
    "\n",
    "def search(query: str, k=1):\n",
    "    encoded_query = query.unsqueeze(dim=0).numpy()\n",
    "    top_k = index.search(encoded_query, k)\n",
    "    gallery_imgs_idxs = top_k[1][0].copy()\n",
    "    top_k[1][0] = np.take(gallery_label, indices=top_k[1][0])\n",
    "    return top_k, gallery_imgs_idxs\n",
    "#     return top_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, std = torch.tensor([0.485, 0.456, 0.406]), torch.tensor([0.229, 0.224, 0.225])\n",
    "t = transforms.Compose([transforms.ToPILImage(), \n",
    "                        transforms.Resize(size=(128,48))\n",
    "                       ])\n",
    "\n",
    "def visualize(query_img, gallery_imgs, gallery_idxs, save_path):\n",
    "    plt.figure(figsize=(16.,6.))\n",
    "\n",
    "    plt.subplot(1,11,1)\n",
    "    img_tensor = query_img.clone()\n",
    "    for i in range(3):\n",
    "        img_tensor[i] = (img_tensor[i] * std[i]) + mean[i]\n",
    "    x = t(img_tensor)\n",
    "    x = np.array(x)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(x)\n",
    "\n",
    "    for j in range(10):\n",
    "        img_tensor = gallery_imgs[gallery_idxs[j]].clone()\n",
    "        for i in range(3):\n",
    "            img_tensor[i] = (img_tensor[i] * std[i]) + mean[i]\n",
    "        x = t(img_tensor)\n",
    "        x = np.array(x)\n",
    "        plt.subplot(1,11,j+2)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(x)\n",
    "        \n",
    "    plt.savefig(save_path)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -r \"/home/shubham/CVP/Predictions\"\n",
    "if not os.path.exists(visualization_path):\n",
    "    os.mkdir(visualization_path)\n",
    "    os.mkdir(os.path.join(visualization_path, \"correct\"))\n",
    "    os.mkdir(os.path.join(visualization_path, \"incorrect\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluate \n",
    "rank1_score = 0\n",
    "rank5_score = 0\n",
    "ap = 0\n",
    "count = 0\n",
    "for query, label in zip(concatenated_query_vectors, query_label):\n",
    "    query_img = query_imgs[count]\n",
    "    \n",
    "    count += 1\n",
    "    label = label\n",
    "    output, gallery_imgs_idxs = search(query, k=10)\n",
    "    # output = search(query, k=10)\n",
    "    \n",
    "    r1 = rank1(label, output) \n",
    "    rank1_score += r1\n",
    "    rank5_score += rank5(label, output) \n",
    "    ap += calc_map(label, output)\n",
    "    \n",
    "    if r1:\n",
    "        # save_path = os.path.join(visualization_path, \"correct\")\n",
    "        # save_path = os.path.join(save_path, str(count-1)+\".png\")\n",
    "        # visualize(query_img, gallery_imgs, gallery_imgs_idxs, save_path)\n",
    "        pass\n",
    "    else:\n",
    "        save_path = os.path.join(visualization_path, \"incorrect\")\n",
    "        save_path = os.path.join(save_path, str(count-1)+\".png\")\n",
    "        visualize(query_img, gallery_imgs, gallery_imgs_idxs, save_path)\n",
    "\n",
    "print(\"Correct: {}, Total: {}, Incorrect: {}\".format(rank1_score, count, count-rank1_score))\n",
    "print(\"Rank1: %.3f, Rank5: %.3f, mAP: %.3f\"%(rank1_score/len(query_feature), \n",
    "                                             rank5_score/len(query_feature), \n",
    "                                             ap/len(query_feature)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_img = query_imgs[1]\n",
    "# output, gallery_imgs_idxs = search(concatenated_query_vectors[1], k=10)\n",
    "# visualize(query_img, gallery_imgs, gallery_imgs_idxs, True)\n",
    "# rank1(query_label[1], output) \n",
    "# rank5(query_label[1], output) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(query_imgs), len(gallery_imgs))\n",
    "# print(len(concatenated_query_vectors), len(concatenated_gallery_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index = faiss.IndexFlatIP(10752)\n",
    "# index.add(np.array([t.numpy() for t in concatenated_gallery_vectors]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index.ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded_query = concatenated_query_vectors[0].unsqueeze(dim=0).numpy()\n",
    "# top_k = index.search(encoded_query, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lbls = np.array(gallery_label)\n",
    "# lbls.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_k[1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.take(lbls, indices=top_k[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gallery_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
